import numpy as np
import json
import os
from robosuite.utils.camera_utils import get_camera_extrinsic_matrix, get_camera_intrinsic_matrix
import robosuite.macros as macros


class PointMapReconstructor:
    """LIBERO 4è§†è§’ RGB-D ç‚¹äº‘é‡å»ºå™¨ - å¸¦ç©ºé—´è¿‡æ»¤"""

    def __init__(self, max_points: int = 30000,
                 spatial_bounds: dict = None):
        """
        Args:
            max_points: æ¯å¸§æœ€å¤§ç‚¹æ•°
            spatial_bounds: ç©ºé—´è¿‡æ»¤èŒƒå›´ï¼Œæ ¼å¼ï¼š
                {
                    'x': (min, max),  # ä¾‹å¦‚ (-0.8, 0.8)
                    'y': (min, max),  # ä¾‹å¦‚ (-0.8, 0.8)
                    'z': (min, max)   # ä¾‹å¦‚ (0.6, 1.5)
                }
                å¦‚æœä¸ºNoneï¼Œåˆ™ä¸è¿‡æ»¤
        """
        self.max_points = max_points
        self.spatial_bounds = spatial_bounds
        self.frames = []
        self.cam_names = None
        self.class_id_to_name = {}

        if spatial_bounds:
            print(f"[PointMap] å¯ç”¨ç©ºé—´è¿‡æ»¤:")
            print(f"  XèŒƒå›´: {spatial_bounds.get('x', 'None')}")
            print(f"  YèŒƒå›´: {spatial_bounds.get('y', 'None')}")
            print(f"  ZèŒƒå›´: {spatial_bounds.get('z', 'None')}")

    def reset(self):
        """æ¸…ç©ºç¼“å­˜ï¼Œå‡†å¤‡å¤„ç†æ–°çš„episode"""
        self.frames = []
        self.class_id_to_name={}
        # cam_namesä¿ç•™ï¼Œé¿å…é‡å¤æ£€æµ‹
        print(f"[PointMap] ç¼“å­˜å·²æ¸…ç©ºï¼Œå‡†å¤‡æ–°episode")

    # def _apply_spatial_filter(self, points: np.ndarray, colors: np.ndarray, labels: np.ndarray):
    #     """åº”ç”¨ç©ºé—´è¿‡æ»¤"""
    #     if self.spatial_bounds is None:
    #         return points, colors, labels
    #
    #     # åˆ›å»ºmask
    #     mask = np.ones(len(points), dtype=bool)
    #
    #     if 'x' in self.spatial_bounds:
    #         x_min, x_max = self.spatial_bounds['x']
    #         mask &= (points[:, 0] >= x_min) & (points[:, 0] <= x_max)
    #
    #     if 'y' in self.spatial_bounds:
    #         y_min, y_max = self.spatial_bounds['y']
    #         mask &= (points[:, 1] >= y_min) & (points[:, 1] <= y_max)
    #
    #     if 'z' in self.spatial_bounds:
    #         z_min, z_max = self.spatial_bounds['z']
    #         mask &= (points[:, 2] >= z_min) & (points[:, 2] <= z_max)
    #
    #     filtered_points = points[mask]
    #     filtered_colors = colors[mask]
    #     filtered_labels = labels[mask]
    #
    #     # if len(points) > 0:
    #     #     filter_rate = 100 * (1 - len(filtered_points) / len(points))
    #     #     # if filter_rate > 50:  # åªåœ¨è¿‡æ»¤è¶…è¿‡50%æ—¶æ‰“å°
    #     #     #     print(f"[PointMap] ç©ºé—´è¿‡æ»¤: {len(points)} â†’ {len(filtered_points)} "
    #     #     #           f"(è¿‡æ»¤æ‰ {filter_rate:.1f}%)")
    #
    #     return filtered_points, filtered_colors, filtered_labels

    # def capture_frame(self, obs: dict, env, timestamp: float, step_idx: int):
    #     """æ•è·ä¸€å¸§4è§†è§’åˆå¹¶ç‚¹äº‘"""
    #     if self.cam_names is None:
    #         self.cam_names = [k.replace('_depth', '') for k in obs.keys() if k.endswith('_depth')]
    #         print(f"[PointMap] æ£€æµ‹åˆ° {len(self.cam_names)} ä¸ªæ·±åº¦ç›¸æœº: {self.cam_names}")
    #
    #     all_points = []
    #     all_colors = []
    #     all_labels = []  # ğŸ”¥ æ–°å¢
    #
    #     for cam_name in self.cam_names:
    #         depth = obs[f'{cam_name}_depth'].squeeze()
    #         rgb = obs[f'{cam_name}_image']
    #         seg = obs[f'{cam_name}_segmentation_instance']  # ğŸ”¥ ç›´æ¥è·å–åˆ†å‰²å›¾
    #
    #         # points, colors = self._depth_to_pointcloud(depth, rgb, cam_name, env)
    #         points, colors, labels = self._depth_to_pointcloud(depth, rgb, seg, cam_name, env)
    #
    #         # åº”ç”¨ç©ºé—´è¿‡æ»¤
    #         # points, colors = self._apply_spatial_filter(points, colors)
    #         points, colors, labels = self._apply_spatial_filter(points, colors, labels)
    #
    #         all_points.append(points)
    #         all_colors.append(colors)
    #         all_labels.append(labels)
    #
    #     merged_points = np.vstack(all_points)
    #     merged_colors = np.vstack(all_colors)
    #     merged_labels = np.concatenate(all_labels)
    #     # ====== æ–°å¢ï¼šåæ ‡ç³»è½¬æ¢ ======
    #     # MuJoCo (x, y, z) â†’ Three.js (x, y, z)
    #     # MuJoCo: x=å³, y=å‰, z=ä¸Š
    #     # Three.js: x=å³, y=ä¸Š, z=å‰
    #     # =============================
    #
    #     # ä¸‹é‡‡æ ·åˆ°max_points
    #     if merged_points.shape[0] > self.max_points:
    #         indices = np.random.choice(merged_points.shape[0], self.max_points, replace=False)
    #         merged_points = merged_points[indices]
    #         merged_colors = merged_colors[indices]
    #         merged_labels = merged_labels[indices]
    #
    #     self.frames.append({
    #         'timestamp': float(timestamp),
    #         'step_idx': int(step_idx),
    #         'points': merged_points.tolist(),
    #         'colors': (merged_colors * 255).astype(np.uint8).tolist(),
    #         'labels': merged_labels.tolist(),
    #     })
    #
    #     # print(f"[PointMap] å¸§ {step_idx}: {merged_points.shape[0]} ç‚¹")
    #     return merged_points.shape[0]
    def _build_class_id_mapping(self, env, first_frame_obs):
        """æ ¹æ®get_segmentation_instancesçš„é€»è¾‘æ„å»ºæ˜ å°„"""

        if hasattr(env, 'segmentation_id_mapping'):
            print("\n=== æ„å»ºIDæ˜ å°„ï¼ˆåŸºäºå®˜æ–¹é€»è¾‘ï¼‰===")

            # å…³é”®ï¼šåˆ†å‰²å›¾ID = segmentation_id_mappingçš„key + 1
            for seg_id, instance_name in env.segmentation_id_mapping.items():
                pixel_id = seg_id + 1  # â† è¿™æ˜¯åˆ†å‰²å›¾ä¸­çš„å®é™…ID

                # æå–ç±»å
                if instance_name in ["OnTheGroundPanda0", "NullMount0"]:
                    class_name = "robot"
                elif "_" in instance_name and instance_name.split("_")[-1].isdigit():
                    class_name = "_".join(instance_name.split("_")[:-1])
                else:
                    class_name = instance_name

                self.class_id_to_name[pixel_id] = class_name
                print(f"  ID {pixel_id} -> {class_name} (from {instance_name})")

            # æ·»åŠ æœºå™¨äººIDï¼ˆå¦‚æœæœ‰ï¼‰
            if hasattr(env, 'segmentation_robot_id') and env.segmentation_robot_id is not None:
                robot_pixel_id = env.segmentation_robot_id + 1
                self.class_id_to_name[robot_pixel_id] = "robot"
                print(f"  ID {robot_pixel_id} -> robot (robot_id)")

            if env.segmentation_id_mapping:
                max_seg_id = max(env.segmentation_id_mapping.keys())
                gripper_pixel_id = max_seg_id + 2  # +1æ˜ å°„åˆ°åˆ†å‰²å›¾ï¼Œå†+1æ˜¯gripper
                self.class_id_to_name[gripper_pixel_id] = "gripper"
                print(f"  ID {gripper_pixel_id} -> gripper (å›ºå®šï¼Œæœ€å¤§ID+2)")

            print(f"[PointMap] è·å–æ˜ å°„: {len(self.class_id_to_name)} ä¸ª")

        # å›ºå®šID 0ä¸ºç¯å¢ƒ
        if 0 not in self.class_id_to_name:
            self.class_id_to_name[0] = "environment"
            print(f"  ID 0 -> environment (å›ºå®š)")

        # æ£€æŸ¥æœªæ˜ å°„çš„ID
        visible_ids = set()
        for cam_name in self.cam_names:
            seg = first_frame_obs[f'{cam_name}_segmentation_instance']
            visible_ids.update(np.unique(seg).tolist())

        unmapped_ids = visible_ids - set(self.class_id_to_name.keys())
        if unmapped_ids:
            print(f"[PointMap] æœªæ˜ å°„çš„ID: {sorted(unmapped_ids)}")
            for uid in unmapped_ids:
                self.class_id_to_name[int(uid)] = f"unknown_{uid}"


    def capture_frame(self, obs: dict, env, timestamp: float, step_idx: int):
        """æ•è·ä¸€å¸§4è§†è§’åˆå¹¶ç‚¹äº‘"""
        if self.cam_names is None:
            self.cam_names = [k.replace('_depth', '') for k in obs.keys() if k.endswith('_depth')]
            print(f"[PointMap] æ£€æµ‹åˆ° {len(self.cam_names)} ä¸ªæ·±åº¦ç›¸æœº: {self.cam_names}")
        if not self.class_id_to_name:
            try:
                self._build_class_id_mapping(env, obs)
                # print("æˆåŠŸä¿å­˜class id")
                # print(f"[PointMap] æ€»æ˜ å°„æ•°: {len(self.class_id_to_name)}")
                #
                # # â† æ·»åŠ è¿™ä¸ªè°ƒè¯•
                # print(f"[PointMap] æ‰€æœ‰keys: {list(self.class_id_to_name.keys())}")
                # print(f"[PointMap] keysç±»å‹: {[type(k) for k in self.class_id_to_name.keys()]}")
                #
                # print("\n========== å®Œæ•´IDæ˜ å°„è¡¨ ==========")
                # for bid in sorted(self.class_id_to_name.keys()):
                #     # æ£€æŸ¥ç±»å‹
                #     print(f"  ID {int(bid):3d} -> {self.class_id_to_name[bid]}")
                # print("==================================\n")
            except Exception as e:
                print(f"ä¿å­˜class idå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        all_points = []
        all_colors_rgb = []  # ğŸ”¥ RGBé¢œè‰²
        all_colors_seg = []  # ğŸ”¥ åˆ†å‰²å›¾é¢œè‰²
        all_labels = []

        for cam_name in self.cam_names:
            depth = obs[f'{cam_name}_depth'].squeeze()
            rgb = obs[f'{cam_name}_image']
            seg = obs[f'{cam_name}_segmentation_instance']

            # ğŸ”¥ è·å–åˆ†å‰²å›¾çš„å¯è§†åŒ–é¢œè‰²
            seg_vis = self._visualize_segmentation(seg, env)

            points, colors_rgb, colors_seg, labels = self._depth_to_pointcloud(
                depth, rgb, seg, seg_vis, cam_name, env
            )

            # ç©ºé—´è¿‡æ»¤
            points, colors_rgb, colors_seg, labels = self._apply_spatial_filter(
                points, colors_rgb, colors_seg, labels
            )

            all_points.append(points)
            all_colors_rgb.append(colors_rgb)
            all_colors_seg.append(colors_seg)
            all_labels.append(labels)

        merged_points = np.vstack(all_points)
        merged_colors_rgb = np.vstack(all_colors_rgb)
        merged_colors_seg = np.vstack(all_colors_seg)
        merged_labels = np.concatenate(all_labels)

        # ä¸‹é‡‡æ ·
        if merged_points.shape[0] > self.max_points:
            indices = np.random.choice(merged_points.shape[0], self.max_points, replace=False)
            merged_points = merged_points[indices]
            merged_colors_rgb = merged_colors_rgb[indices]
            merged_colors_seg = merged_colors_seg[indices]
            merged_labels = merged_labels[indices]

        self.frames.append({
            'timestamp': float(timestamp),
            'step_idx': int(step_idx),
            'points': merged_points.tolist(),
            'colors_rgb': (merged_colors_rgb * 255).astype(np.uint8).tolist(),  # ğŸ”¥ RGBé¢œè‰²
            'colors_seg': (merged_colors_seg * 255).astype(np.uint8).tolist(),  # ğŸ”¥ åˆ†å‰²å›¾é¢œè‰²
            'labels': merged_labels.tolist(),
        })

        return merged_points.shape[0]

    # def _depth_to_pointcloud(self, depth: np.ndarray, rgb: np.ndarray, seg: np.ndarray, cam_name: str, env):
    #     """ä½¿ç”¨robosuiteå·¥å…·è¿›è¡Œæ·±åº¦åæŠ•å½±"""
    #     sim = env.sim
    #     H, W = depth.shape
    #     # æ£€æŸ¥å›¾åƒçº¦å®šï¼ˆOpenGLæ˜¯flippedï¼Œéœ€è¦ä¸Šä¸‹ç¿»è½¬ï¼‰
    #     is_opengl_flipped = (macros.IMAGE_CONVENTION == "opengl")
    #
    #     # å½’ä¸€åŒ–æ·±åº¦ -> çœŸå®è·ç¦»
    #     model = sim.model
    #     extent = model.stat.extent
    #     near = model.vis.map.znear * extent
    #     far = model.vis.map.zfar * extent
    #     depth_real = near / (1.0 - depth * (1.0 - near / far))
    #
    #     # å¦‚æœæ˜¯OpenGLçº¦å®šï¼Œä¸Šä¸‹ç¿»è½¬æ·±åº¦å›¾å’ŒRGB
    #     if is_opengl_flipped:
    #         depth_real = np.flipud(depth_real)
    #         rgb = np.flipud(rgb)
    #         seg = np.flipud(seg)  # ğŸ”¥ åˆ†å‰²å›¾ä¹Ÿè¦ç¿»è½¬
    #
    #     # ä½¿ç”¨robosuiteè·å–å†…å‚å’Œå¤–å‚
    #     K = get_camera_intrinsic_matrix(sim, cam_name, H, W)
    #     fx, fy = K[0, 0], K[1, 1]
    #     cx, cy = K[0, 2], K[1, 2]
    #
    #     # robosuiteçš„get_camera_extrinsic_matrixè¿”å›çš„æ˜¯cam_to_world
    #     cam_to_world = get_camera_extrinsic_matrix(sim, cam_name)
    #
    #     # åƒç´ ç½‘æ ¼
    #     u, v = np.meshgrid(np.arange(W), np.arange(H))
    #     u = u.flatten()
    #     v = v.flatten()
    #     z = depth_real.flatten()
    #     seg_flat = seg.flatten()
    #
    #     # æ·±åº¦è¿‡æ»¤
    #     valid = (z > 0.1) & (z < 5.0) & np.isfinite(z)
    #     u = u[valid]
    #     v = v[valid]
    #     z = z[valid]
    #     colors = rgb.reshape(-1, 3)[valid]
    #     labels = seg_flat[valid]
    #
    #     # åƒç´ åæ ‡ -> ç›¸æœºåæ ‡ï¼ˆOpenCVçº¦å®šï¼š+Zå‰æ–¹ï¼‰
    #     x_cam = (u - cx) * z / fx
    #     y_cam = (v - cy) * z / fy
    #     z_cam = z
    #     points_cam_homo = np.stack([x_cam, y_cam, z_cam, np.ones(len(z))], axis=1)
    #
    #     # ç›¸æœºåæ ‡ -> ä¸–ç•Œåæ ‡
    #     points_world_homo = (cam_to_world @ points_cam_homo.T).T
    #     points_world = points_world_homo[:, :3]
    #
    #     return points_world, colors, labels
    def _depth_to_pointcloud(self, depth: np.ndarray, rgb: np.ndarray,
                             seg: np.ndarray, seg_vis: np.ndarray, cam_name: str, env):
        """æ·±åº¦åæŠ•å½±ï¼Œè¿”å›RGBé¢œè‰²å’Œåˆ†å‰²é¢œè‰²"""
        sim = env.sim
        H, W = depth.shape

        if len(seg.shape) == 3:
            seg = seg.squeeze()

        is_opengl_flipped = (macros.IMAGE_CONVENTION == "opengl")

        # å½’ä¸€åŒ–æ·±åº¦
        model = sim.model
        extent = model.stat.extent
        near = model.vis.map.znear * extent
        far = model.vis.map.zfar * extent
        depth_real = near / (1.0 - depth * (1.0 - near / far))

        # OpenGLç¿»è½¬
        if is_opengl_flipped:
            depth_real = np.flipud(depth_real)
            rgb = np.flipud(rgb)
            seg = np.flipud(seg)
            seg_vis = np.flipud(seg_vis)  # ğŸ”¥ åˆ†å‰²å›¾å¯è§†åŒ–ä¹Ÿè¦ç¿»è½¬

        # ç›¸æœºå‚æ•°
        K = get_camera_intrinsic_matrix(sim, cam_name, H, W)
        fx, fy = K[0, 0], K[1, 1]
        cx, cy = K[0, 2], K[1, 2]
        cam_to_world = get_camera_extrinsic_matrix(sim, cam_name)

        # åƒç´ ç½‘æ ¼
        u, v = np.meshgrid(np.arange(W), np.arange(H))
        u = u.flatten()
        v = v.flatten()
        z = depth_real.flatten()
        seg_flat = seg.flatten()

        # æ·±åº¦è¿‡æ»¤
        valid = (z > 0.1) & (z < 5.0) & np.isfinite(z)
        u = u[valid]
        v = v[valid]
        z = z[valid]
        colors_rgb = rgb.reshape(-1, 3)[valid] / 255.0  # ğŸ”¥ å½’ä¸€åŒ–åˆ°0-1
        colors_seg = seg_vis.reshape(-1, 3)[valid]  # ğŸ”¥ åˆ†å‰²é¢œè‰²ï¼Œå·²ç»æ˜¯0-1
        labels = seg_flat[valid]

        # åæŠ•å½±
        x_cam = (u - cx) * z / fx
        y_cam = (v - cy) * z / fy
        z_cam = z
        points_cam_homo = np.stack([x_cam, y_cam, z_cam, np.ones(len(z))], axis=1)

        points_world_homo = (cam_to_world @ points_cam_homo.T).T
        points_world = points_world_homo[:, :3]

        return points_world, colors_rgb, colors_seg, labels  # ğŸ”¥ è¿”å›4ä¸ªå€¼

    def _apply_spatial_filter(self, points: np.ndarray, colors_rgb: np.ndarray,
                              colors_seg: np.ndarray, labels: np.ndarray):
        """ç©ºé—´è¿‡æ»¤"""
        if self.spatial_bounds is None:
            return points, colors_rgb, colors_seg, labels

        mask = np.ones(len(points), dtype=bool)

        if 'x' in self.spatial_bounds:
            x_min, x_max = self.spatial_bounds['x']
            mask &= (points[:, 0] >= x_min) & (points[:, 0] <= x_max)

        if 'y' in self.spatial_bounds:
            y_min, y_max = self.spatial_bounds['y']
            mask &= (points[:, 1] >= y_min) & (points[:, 1] <= y_max)

        if 'z' in self.spatial_bounds:
            z_min, z_max = self.spatial_bounds['z']
            mask &= (points[:, 2] >= z_min) & (points[:, 2] <= z_max)

        return points[mask], colors_rgb[mask], colors_seg[mask], labels[mask]

    def _visualize_segmentation(self, seg: np.ndarray, env):
        """å°†åˆ†å‰²å›¾è½¬æ¢ä¸ºå½©è‰²å¯è§†åŒ–å›¾åƒï¼ˆè¿”å›float 0-1ï¼‰"""
        if len(seg.shape) == 3:
            seg = seg.squeeze()

        H, W = seg.shape
        vis_img = np.zeros((H, W, 3), dtype=np.float32)  # ğŸ”¥ float32ï¼ŒèŒƒå›´0-1

        unique_ids = np.unique(seg)
        for uid in unique_ids:
            mask = (seg == uid)
            hue = (uid * 137.508) % 360
            r, g, b = self._hsl_to_rgb(hue / 360, 0.8, 0.6)
            vis_img[mask] = [r, g, b]

        return vis_img

    def _hsl_to_rgb(self, h, s, l):
        """HSLè½¬RGBï¼ˆè¿”å›0-1èŒƒå›´ï¼‰"""
        if s == 0:
            r = g = b = l
        else:
            def hue2rgb(p, q, t):
                if t < 0: t += 1
                if t > 1: t -= 1
                if t < 1 / 6: return p + (q - p) * 6 * t
                if t < 1 / 2: return q
                if t < 2 / 3: return p + (q - p) * (2 / 3 - t) * 6
                return p

            q = l * (1 + s) if l < 0.5 else l + s - l * s
            p = 2 * l - q
            r = hue2rgb(p, q, h + 1 / 3)
            g = hue2rgb(p, q, h)
            b = hue2rgb(p, q, h - 1 / 3)

        return r, g, b

    def save_frames_as_json(self, output_dir: str, episode_id: int, env=None):
        """ä¿å­˜ç‚¹äº‘å¸§åºåˆ—ä¸ºJSON"""
        os.makedirs(output_dir, exist_ok=True)
        # ğŸ”¥ å¦‚æœä¼ å…¥äº†envï¼Œå°±ä¿å­˜siteæ˜ å°„
        site_mapping = {}
        if env is not None:
            for site_id in range(env.sim.model.nsite):
                name = env.sim.model.site_id2name(site_id)
                if name:
                    site_mapping[site_id] = name

        metadata = {
            'episode_id': episode_id,
            'total_frames': len(self.frames),
            'max_points_per_frame': self.max_points,
            'cameras': self.cam_names,
            # 'site_mapping': site_mapping  # ğŸ”¥ ç›´æ¥ä¿å­˜åœ¨metadataé‡Œ
            'class_id_to_name': self.class_id_to_name,  # è€Œä¸æ˜¯ 'site_mapping'
        }

        meta_path = f"{output_dir}/pointmeta_ep_{episode_id}.json"
        with open(meta_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        frames_path = f"{output_dir}/pointframes_ep_{episode_id}.json"
        print(f"[PointMap] æ­£åœ¨ä¿å­˜ {len(self.frames)} å¸§åˆ° {frames_path}...")
        with open(frames_path, 'w') as f:
            json.dump(self.frames, f)

        size_mb = os.path.getsize(frames_path) / 1024 / 1024
        print(f"[PointMap] å·²ä¿å­˜ -> {frames_path} ({size_mb:.1f} MB)")
        return meta_path, frames_path

    def get_summary(self):
        """è¿”å›é‡å»ºæ‘˜è¦"""
        if not self.frames:
            return {"error": "æ²¡æœ‰æ•è·ä»»ä½•å¸§"}

        point_counts = [len(f['points']) for f in self.frames]
        duration = self.frames[-1]['timestamp'] - self.frames[0]['timestamp']

        return {
            'total_frames': len(self.frames),
            'avg_points_per_frame': int(np.mean(point_counts)),
            'max_points_per_frame': max(point_counts),
            'min_points_per_frame': min(point_counts),
            'duration_seconds': float(duration),
            'cameras': self.cam_names
        }